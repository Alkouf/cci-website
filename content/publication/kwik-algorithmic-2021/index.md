---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'Algorithmic fog of war: When lack of transparency violates the law of armed
  conflict'
subtitle: ''
summary: ''
authors:
- Jonathan Kwik
- tom
tags: []
categories: []
date: '2021-12-01'
lastmod: 2022-05-31T10:27:39+02:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-05-31T08:27:39.143995Z'
publication_types:
- '2'
abstract: Under international law, weapon capabilities and their use are regulated
  by legal requirements set by International Humanitarian Law (IHL). Currently, there
  are strong military incentives to equip capabilities with increasingly advanced
  artificial intelligence (AI), which include opaque (less transparent) models. As
  opaque models sacrifice transparency for performance, it is necessary to examine
  whether their use remains in conformity with IHL obligations. First, we demonstrate
  that the incentives for automation drive AI toward complex task areas and dynamic
  and unstructured environments, which in turn necessitates resort to more opaque
  solutions. We subsequently discuss the ramifications of opaque models for foreseeability
  and explainability. Then, we analyse their impact on IHL requirements from a development,
  pre-deployment and post-deployment perspective. We find that while IHL does not
  regulate opaque AI directly, the lack of foreseeability and explainability frustrates
  the fulfilment of key IHL requirements to the extent that the use of fully opaque
  AI could violate international law. States are urged to implement interpretability
  during development and seriously consider the challenging complication of determining
  the appropriate balance between transparency and performance in their capabilities.
publication: '*Journal of Future Robot Life*'
doi: 10.3233/FRL-200019
links:
- name: URL
  url: https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/FRL-200019
---
